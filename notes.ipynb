{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Neural Tangent Kernels\n","\n","A few notes on the theory behind NTKs, based on  \n","- [Belkin et al, 2018](https://arxiv.org/abs/1812.11118)\n","-  [Jacot et al, 2018](http://arxiv.org/abs/1806.07572)\n","- [Lee et al, 2018](https://arxiv.org/abs/1711.00165)\n","-  [Lee et al, 2019](http://arxiv.org/abs/1902.06720)  \n","- [Chizat et al,2019](https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf)\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","from scipy.linalg import expm "]},{"cell_type":"markdown","metadata":{},"source":["\n","## Background \n","\n","- unclear why and how neural networks generalise so well\n","- more or less impossible to understand learning dynamics in nonlinear network  \n","\n","### Bias/Variance Trade-Off in Neural networks (Belkin et al)\n","- bias/variance trade-off doesn't explain behaviour of neural networks:\n","- if increase size even beyound threshold required for convergence on training data, test error begins to drop again   \n","\n","![u-shaped loss](./belkin.jpg)  \n","\n","- authors introduce notion of double decent \n","- motivation for study of networks in inifinte width limit \n","\n","\n","### Infinite Width Neural Networks & Gaussian Processes  \n","In the infinite width limit, function specified by neural network can be understood as function drawn from a Gaussian Process.  \n","#### Background Reading:  \n","- Equivalence of infinite width random Nnets and GPs: [Neal, 1994a](https://www.cs.toronto.edu/~radford/ftp/pin.pdf)  \n","- Extension to Deep Nets: [Lee et al, 2018](https://arxiv.org/abs/1711.00165)    \n","- [my implementation of a Gaussian Process from scratch](https://github.com/TimoFlesch/NTK/blob/master/gaussian_process.ipynb)  \n","\n","Intuitive Explanation: Weights and biases are gaussian random variables. The sum of all inputs into a unit (i.e. its preactivations) is thus a sum of gaussian RVs which itself is a gaussian RV (CLT).     \n","\n","\n","#### Formally speaking\n","If $z_i = f_{i-1}(x_{i-1},\\theta)$ denotes the preactivations at layer $i$, then \n","$$\n","z_i \\sim \\mathcal{GP}(m(z(x)),K(z(x),z(x')))\n","$$\n"," as width goes to infinity.  \n","* Parameters have zero mean, i.e. \n","$$m(z) = \\mathbb{E}(z(x)) = 0$$\n","* Covariance matrix is given by kernel $$\\kappa(x,x') = \\langle z(x)z(x')\\rangle$$\n","* Can be extended to deep nets via induction  \n","\n","#### Interpretation\n","This is very cool from historical perspective: Nnets gained popularity in the 80s but computationally too demanding. Kernel methods took over. Now, nnets again on vogue. Good to know that there is a connection.\n","\n","#### Problem\n","This only holds for a randomly initialised network where only the readout layer is trained. Therefore, doesn't really help with understanding SGD training dynamics\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","## Neural Tangent Kernels \n","### *Neural Tangent Kernel (Jacot et al,2018)*  \n","#### Goal\n","   \n","We'd like to understand the learning dynamics of a neural network. Know great deal about dynamics in linear case (thanks Andrew!), but SGD in nonlinear network very hard to study.   \n","The NTK is a kernel for nnets trained end-to-end with SGD. It applies to networks in the infinite width limit (or more generally where the network is close to its linear approximation) Authors demonstrated that NTK at init is good approximation for time-varying NTK  and thus dynamics can be analytically described as long as network operates in kernel regime. \n","\n","#### Gradient Flows: Derivation of NTK\n","Let's have a look at training dynamics under Gradient Descent. Standard Formula:    \n","$$\n","\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_\\theta L(\\theta_t) \n","$$\n","$$\n","\\frac{\\theta_{t+1}-\\theta{t}}{\\eta} = -\\nabla_\\theta L(\\theta_t)\n","$$\n","We assume an L2 loss:\n","$$\n","L(\\theta) = \\frac{1}{2}||y(\\theta,x)-\\hat{y}||\n","$$\n","If we assume an infinitesimally small learning rate $\\eta$, we can interpret this as differential equation that tracks the change of the parameter vectors over time (**Gradient Flow**):\n","$$ \n","\\frac{d\\theta(t)}{dt} = \\dot{\\theta} = -\\nabla_\\theta L(\\theta_t) \n","$$\n","$$\n"," \\dot{\\theta} = -\\nabla_\\theta y(\\theta)(y(\\theta)-\\hat{y})\n","$$\n","Now here comes the cool part: We're dealing with a neural network, so the chain rule of calculus applies! This means that we can derive the dynamics of the model outputs (i.e. dynamics in function space) induced by the dynamics of the weights very easily:\n","$$\n","\\dot{y(\\theta)} = \\nabla_\\theta y(\\theta)^T\\dot{\\theta} \n","$$\n","$$\n","= -\\nabla_\\theta y(\\theta)^T\\nabla_\\theta y(\\theta)(y(\\theta)-\\hat{y})\n","$$\n","Now, we assume that the gradient of the network function acts as a feature map $\\phi(x)$, which implies that its dot product over all the data points is a valid kernel, the Neural Tangent Kernel (NTK):\n","$$ \n","H(t) = \\kappa_{NTK}(x,x') =  \\langle -\\nabla_\\theta y(\\theta(t),x)^T\\nabla_\\theta y(\\theta(t),x) \\rangle, \\forall x,x' \\in \\mathbb{R}^d \n","$$\n","\n","#### Lazy Learning: NTK and the Infinite Width Limit\n","In the infinite width limit, we make an interesting observation: Weights change only very little during training, i.e. stay close to their random initialisation:\n","$$\n","\\frac{||\\theta(t)-\\theta(0)||_2}{||\\theta(0)||_2} \\approx 0\n","$$ \n","And hence, the kernel barely varies during training:\n","$$\n","\\kappa_t(x,x') \\approx \\kappa_{t}(x,x'), \\forall t \n","$$ \n","Let's come up with an intuitive explanation of this phenomenon: With infinitely many weights, very small changes lead to a big **net change** in the preactivations (i.e. linearity) of a hidden layer neuron. In contrast, with only very few weights, these have to change substantially to yield the same effect.  \n","Moreover, Jacot has shown that the kernel at initialisation, which depends on a random variable (weights drawn from iid gaussian) converges in probability to a deterministic value in the large weight limit, i.e. \n","$$\n","\\kappa_0(x,x') \\approx \\kappa_{NTK}(x,x')\n","$$\n","So now we know that the time-varying kernel is essentially identical to the NTK. Importantly, the network trained to convergence is equivalent to the kernel regression solution with an NTK\n","$$\n","f_{SGD}(x,\\theta) \\approx f_{NTK}(x,\\theta) = \\kappa_{NTK}(x,X)^T \\kappa_{NTK}(X,X)^{-1}y\n","$$\n","where $X$ corresponds to the dataset s.t. $\\kappa_{NTK}(x,X) = [\\kappa_{NTK}(x,x_1),...,\\kappa_{NTK}(x,x_n)]^T$\n","\n","\n","#### Linear Approximation of Infinite Width Network (Lee et al,2019)  \n","At this point, it's still not clear to me how to study the dynamics of the network. However, we know that - in the NTK regime - the network parameters barely change during learning. Insight: We can just apply a 1st-order Taylor expansion (hence a linear approximation) to the network function w.r.t. its parameters around its initialisation. Now, the non-linear network is a linear function of the weights, yet still non-linear in its inputs:\n","$$\n","f(x,\\theta) \\approx f(x,\\theta_0) + \\nabla_\\theta f(x,\\theta_0)^T(\\theta-\\theta_0)\n","$$\n","And here comes the connection to NTKs: We can understand this as a linear function of x with a non-linear basis function (feature map) of the inputs, which is simply the gradient on the weights:\n","$$\n","\\phi(x) = \\nabla_\\theta f(x,\\theta_0)\n","$$\n","so the function becomes: \n","$$\n","f(x,\\theta) \\approx f(x,\\theta_0) + \\phi(x)^T(\\theta-\\theta_0)\n","$$\n","This feature map can be understood as a kernel on the inputs, for which we know its form:\n","$$\n","\\kappa(x,x') = \\phi(x)^T\\phi(x') = \\nabla_\\theta f(x,\\theta_0)^T \\nabla_\\theta f(x,\\theta_0)\n","$$\n","\n","Now, the training dynamics are tractable, as we have a linear function of the weights.\n","Todo: brush up my understanding of dual view and the kernel trick (https://alliance.seas.upenn.edu/~cis520/dynamic/2017/wiki/index.php?n=Lectures.Kernels#toc5) \n","\n","#### Analytic Solution to the training dynamics \n","So far, I have understood that the training dynamics can be described by a gradient flow. The expression for this contains the NTK. However, the NTK itself is still dependent on t, i.e. a time-varying process. From the theory of infinite width networks, we known that weights of infinitely wide networks barely change during training (see my intuitive explanation above) and that the network can be approximated by its linearisation. Hence, when in the kernel regime (i.e. barely changing Jacobian, i.e. close to its linearisation) the time-varying kernel of the neural network can be approximated by the kernel at initialisation \n","$$\n","\\nabla y(\\theta(t)) \\approx \\nabla y(\\theta_0)\n","$$ \n","$$\n","H(\\theta(t)) \\approx H(\\theta_0)\n","$$\n","\n","This is very good news, as we can update our expression for the gradient flow, which used to be:\n","$$\n","\\dot{y(\\theta)} = \\nabla_\\theta y(\\theta)^T\\dot{\\theta} = H(\\theta(t)) y(\\theta)-\\hat{y})\n","$$\n","Now, it becomes  \n","$$\n","\\dot{y(\\theta)} = \\nabla_\\theta y(\\theta)^T\\dot{\\theta} = H(\\theta_0) y(\\theta)-\\hat{y})\n","$$\n","**The equilibrium for this equation is achieved when the training loss is zero.**\n","Let's simplify this expression with a substitution of variables\n","$$ \n","u = y(\\theta)-\\hat{y}\n","$$ \n","Now the ODE becomes \n","$$ \n"," \\dot{u} = -H(\\theta_0)u \n","$$ \n","Ok, cool. A system of differential equations can be solved with the matrix exponential. So let's derive the output (recall: difference between output and predictions to be precise) at time t \n","$$ \n","y(\\theta(t))-\\hat{y} = u(t) = u(0)e^{-H(\\theta_0)t}\n","$$ \n","Awesome, we're now able to track the learning dynamics of a neural network using the NTK at initialisation! And we have shown that the SGD converges to zero training loss as long as the non-linear model is close to its linearisation (i.e. in the infinite weight limit)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Lazy Learning and NTK Regime on standard Neural Networks (chizat et al, 2019)\n","\n","All above is pretty cool, but only holds in the infinite width limit. I understand intuitively why. With many weights, each contributes a little, thus ensemble has large impact on output, thus no need for dramatic changes (highly distributed code).\n","But in reality, width is obs. finite. How to apply these findings to real nnets?\n","**Chizat et al argue that a finite-width network can be pushed into the NTK regime by chosing an appropriate explicit scaling factor.**They demonstrate that any parametric model (not only nnets) can be trained in lazy regime if its outputs are close to zero after random initialisation. \n","If I understand correctly, this can be achieved with LeCun initialisation (variance scales with network width)"]},{"cell_type":"markdown","metadata":{},"source":["## A practical example"]},{"cell_type":"markdown","metadata":{},"source":["Let's consider a practical example. \n","TODO- Work in progress"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":2}